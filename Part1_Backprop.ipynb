{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.6"},"colab":{"name":"Part1_Backprop.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"woTEznGbwMQU"},"source":["$$\n","\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n","\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n","\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n","\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n","\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n","\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n","\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n","\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n","\\newcommand{\\set}[1]{\\mathbb {#1}}\n","\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n","\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n","\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n","$$\n","# Part 1: Backpropagation\n","<a id=part1></a>"]},{"cell_type":"markdown","metadata":{"id":"7CgJmb_VwMQb"},"source":["In this part we will learn about **backpropagation** and **automatic differentiation**. We'll implement both of these concepts from scratch and compare our implementation to `PyTorch`'s built in implementation (`autograd`)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T-IDO50QwXM-","executionInfo":{"status":"ok","timestamp":1639405724570,"user_tz":-120,"elapsed":21575,"user":{"displayName":"Shir Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxzgFCwcO2VgDEyrWnLAS5HsXbQDYe4JILsIFVgN4=s64","userId":"04622715026667402457"}},"outputId":"d7a4a7fa-ede0-4741-b442-8d7b553035d5"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ashBtbF6wZ7s","executionInfo":{"status":"ok","timestamp":1639405727251,"user_tz":-120,"elapsed":786,"user":{"displayName":"Shir Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxzgFCwcO2VgDEyrWnLAS5HsXbQDYe4JILsIFVgN4=s64","userId":"04622715026667402457"}},"outputId":"dc2648c1-3ea4-4494-ba38-e3b0b6e92260"},"source":["%cd 'gdrive/My Drive/Deep_Learning_hw_2'\n","\n","! ls"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/.shortcut-targets-by-id/1hsxfu2HUzpCM2mkm35gX8k7Wl2e1FbQx/Deep_Learning_hw_2\n","cs3600\timgs\t Part0_Intro.ipynb     Part2_Optimization.ipynb  results\n","hw2\tmain.py  Part1_Backprop.ipynb  Part3_CNNs.ipynb\t\t tests\n"]}]},{"cell_type":"code","metadata":{"id":"kuilzVhnwMQc","executionInfo":{"status":"ok","timestamp":1639405734934,"user_tz":-120,"elapsed":5772,"user":{"displayName":"Shir Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxzgFCwcO2VgDEyrWnLAS5HsXbQDYe4JILsIFVgN4=s64","userId":"04622715026667402457"}}},"source":["import torch\n","import unittest\n","\n","%load_ext autoreload\n","%autoreload 2\n","\n","test = unittest.TestCase()"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zFeFv936wMQe"},"source":["The backpropagation algorithm is at the core of training deep models. To state the problem we'll tackle in this notebook, imagine we have an L-layer MLP model, defined as\n","$$\n","\\hat{\\vec{y}^i} = \\vec{y}_L^i= \\varphi_L \\left(\n","\\mat{W}_L \\varphi_{L-1} \\left( \\cdots\n","\\varphi_1 \\left( \\mat{W}_1 \\vec{x}^i + \\vec{b}_1 \\right)\n","\\cdots \\right)\n","+ \\vec{b}_L \\right),\n","$$\n","\n","a pointwise loss function $\\ell(\\vec{y}, \\hat{\\vec{y}})$ and an empirical loss over our entire data set,\n","$$\n","L(\\vec{\\theta}) = \\frac{1}{N} \\sum_{i=1}^{N} \\ell(\\vec{y}^i, \\hat{\\vec{y}^i}) + R(\\vec{\\theta})\n","$$\n","\n","where $\\vec{\\theta}$ is a vector containing all network parameters, e.g.\n","$\\vec{\\theta} = \\left[ \\mat{W}_{1,:}, \\vec{b}_1, \\dots,  \\mat{W}_{L,:}, \\vec{b}_L \\right]$."]},{"cell_type":"markdown","metadata":{"id":"MKggy98ywMQf"},"source":["In order to train our model we would like to calculate the derivative\n","(or **gradient**, in the multivariate case) of the loss with respect to each and every one of the parameters,\n","i.e. $\\pderiv{L}{\\mat{W}_j}$ and $\\pderiv{L}{\\vec{b}_j}$ for all $j$.\n","Since the gradient \"points\" to the direction of functional increase, the negative gradient is often used as a descent direction for descent-based optimization algorithms.\n","In other words, iteratively updating each parameter proportianally to it's negetive gradient can lead to\n","convergence to a local minimum of the loss function."]},{"cell_type":"markdown","metadata":{"id":"mMbXaP7nwMQg"},"source":["Calculus tells us that as long as we know the derivatives of all the functions \"along the way\"\n","($\\varphi_i(\\cdot),\\ \\ell(\\cdot,\\cdot),\\ R(\\cdot)$)\n","we can use the **chain rule** to calculate the derivative \n","of the loss with respect to any one of the parameter vectors.\n","Note that if the loss $L(\\vec{\\theta})$ is scalar (which is usually the case), the gradient of a parameter\n","will have the same shape as the parameter itself (matrix/vector/tensor of same dimensions).\n","\n","For deep models that are a composition of many functions, calculating the gradient of each parameter by hand and implementing hard-coded gradient derivations quickly becomes infeasible.\n","Additionally, such code makes models hard to change, since any change potentially requires re-derivation and re-implementation of the entire gradient function.\n","\n","The backpropagation algorithm, which we saw in the lecture, provides us with a effective method of applying the **chain rule** recursively so that we can implement gradient calculations of arbitrarily deep or complex models."]},{"cell_type":"markdown","metadata":{"id":"LpICw90CwMQh"},"source":["We'll now implement backpropagation using a modular approach, which will allow us to chain many components layers together and get automatic gradient calculation of the output with respect to the input or any intermediate parameter."]},{"cell_type":"markdown","metadata":{"id":"T1m_CjjywMQi"},"source":["To do this, we'll define a `Layer` class. Here's the API of this class:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0VLUFv4RwMQj","executionInfo":{"status":"ok","timestamp":1639405739584,"user_tz":-120,"elapsed":1515,"user":{"displayName":"Shir Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxzgFCwcO2VgDEyrWnLAS5HsXbQDYe4JILsIFVgN4=s64","userId":"04622715026667402457"}},"outputId":"5d5b9da6-5d7b-4b6e-a1ab-89f9d1a5ef9f"},"source":["import hw2.layers as layers\n","help(layers.Layer)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Help on class Layer in module hw2.layers:\n","\n","class Layer(abc.ABC)\n"," |  A Layer is some computation element in a network architecture which\n"," |  supports automatic differentiation using forward and backward functions.\n"," |  \n"," |  Method resolution order:\n"," |      Layer\n"," |      abc.ABC\n"," |      builtins.object\n"," |  \n"," |  Methods defined here:\n"," |  \n"," |  __call__(self, *args, **kwargs)\n"," |      Call self as a function.\n"," |  \n"," |  __init__(self)\n"," |      Initialize self.  See help(type(self)) for accurate signature.\n"," |  \n"," |  __repr__(self)\n"," |      Return repr(self).\n"," |  \n"," |  backward(self, dout)\n"," |      Computes the backward pass of the layer, i.e. the gradient\n"," |      calculation of the final network output with respect to each of the\n"," |      parameters of the forward function.\n"," |      :param dout: The gradient of the network with respect to the\n"," |      output of this layer.\n"," |      :return: A tuple with the same number of elements as the parameters of\n"," |      the forward function. Each element will be the gradient of the\n"," |      network output with respect to that parameter.\n"," |  \n"," |  forward(self, *args, **kwargs)\n"," |      Computes the forward pass of the layer.\n"," |      :param args: The computation arguments (implementation specific).\n"," |      :return: The result of the computation.\n"," |  \n"," |  params(self)\n"," |      :return: Layer's trainable parameters and their gradients as a list\n"," |      of tuples, each tuple containing a tensor and it's corresponding\n"," |      gradient tensor.\n"," |  \n"," |  train(self, training_mode=True)\n"," |      Changes the mode of this layer between training and evaluation (test)\n"," |      mode. Some layers have different behaviour depending on mode.\n"," |      :param training_mode: True: set the model in training mode. False: set\n"," |      evaluation mode.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors defined here:\n"," |  \n"," |  __dict__\n"," |      dictionary for instance variables (if defined)\n"," |  \n"," |  __weakref__\n"," |      list of weak references to the object (if defined)\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data and other attributes defined here:\n"," |  \n"," |  __abstractmethods__ = frozenset({'backward', 'forward', 'params'})\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"9LHW2184wMQj"},"source":["In other words, a `Layer` can be anything: a layer, an activation function, a loss function or generally *any computation that we know how to derive a gradient for*.\n","\n","Each block must define a `forward()` function and a `backward()` function.\n","- The `forward()` function performs the actual calculation/operation of the block and returns an output.\n","- The `backward()` function computes the gradient of the **input and parameters** as a function of the gradient of the **output**, according to the chain rule."]},{"cell_type":"markdown","metadata":{"id":"T1SGSW3QwMQk"},"source":["Here's a diagram illustrating the above explanation:\n","\n","<img src=\"imgs/backprop.png\" width=\"900\" />"]},{"cell_type":"markdown","metadata":{"id":"VPNEKgqBwMQk"},"source":["Note that the diagram doesn't show that if the function is parametrized, i.e. $f(\\vec{x},\\vec{y})=f(\\vec{x},\\vec{y};\\vec{w})$, there are also gradients to calculate for the parameters $\\vec{w}$."]},{"cell_type":"markdown","metadata":{"id":"DZcYeFQbwMQl"},"source":["The forward pass is straightforward: just do the computation.\n","To understand the backward pass, imagine that there's some \"downstream\" loss function\n","$L(\\vec{\\theta})$ and magically somehow we are told the gradient of that loss with respect\n","to the **output** $\\vec{z}$ of our block, i.e. $\\pderiv{L}{\\vec{z}}$."]},{"cell_type":"markdown","metadata":{"id":"e6uk3aVJwMQl"},"source":["Now, since we know how to calculate the derivative of $f(\\vec{x},\\vec{y};\\vec{w})$,\n","it means we know how to calculate $\\pderiv{\\vec{z}}{\\vec{x}}$, $\\pderiv{\\vec{z}}{\\vec{y}}$ and $\\pderiv{\\vec{z}}{\\vec{w}}$ .\n","Thanks to the chain rule, this is all we need to calculate the gradients of the **loss** w.r.t. the input and\n","parameters:\n","\n","$$\n","\\begin{align}\n","\\pderiv{L}{\\vec{x}} &= \\pderiv{L}{\\vec{z}}\\cdot \\pderiv{\\vec{z}}{\\vec{x}}\\\\\n","\\pderiv{L}{\\vec{y}} &= \\pderiv{L}{\\vec{z}}\\cdot \\pderiv{\\vec{z}}{\\vec{y}}\\\\\n","\\pderiv{L}{\\vec{w}} &= \\pderiv{L}{\\vec{z}}\\cdot \\pderiv{\\vec{z}}{\\vec{w}}\n","\\end{align}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"9Hiv2k4RwMQl"},"source":["## Comparison with PyTorch\n","<a id=part1_1></a>\n","\n","PyTorch has the [`nn.Module`](https://pytorch.org/docs/stable/nn.html#module) base class, which may seem to be similar to our `Layer` since it also represents a computation element in a network.\n","However PyTorch's `nn.Module`s don't compute the gradient directly, they only define the forward calculations.\n","Instead, PyTorch has a more low-level API for defining a function and explicitly implementing it's `forward()` and `backward()`. See [`autograd.Function`](https://pytorch.org/docs/stable/autograd.html#function).\n","When an operation is performed on a tensor, it creates a `Function` instance which performs the operation and\n","stores any necessary information for calculating the gradient later on. Additionally, `Functions`s point to the \n","other `Function` objects representing the operations performed earlier on the tensor. Thus, a graph (or DAG)\n","of operations is created (this is not 100% exact, as the graph is actually composed of a different type of class which wraps the backward method, but it's accurate enough for our purposes).\n","\n","A `Tensor` instance which was created by performing operations on one or more tensors with `requires_grad=True`, has a `grad_fn` property which is a `Function` instance representing the last operation performed to produce this tensor.\n","This exposes the graph of `Function` instances, each with it's own `backward()` function. Therefore, in PyTorch the `backward()` function is called on the tensors, not the modules."]},{"cell_type":"markdown","metadata":{"id":"FeT-YdqmwMQm"},"source":["Our `Layer`s are therefore a combination of the ideas in `Module` and `Function` and we'll implement them together,\n","just to make things simpler.\n","Our goal here is to create a \"poor man's autograd\": We'll use PyTorch tensors,\n","but we'll calculate and store the gradients in our `Layer`s (or return them).\n","The gradients we'll calculate are of the entire block, not individual operations on tensors.\n","\n","To test our implementation, we'll use PyTorch's `autograd`.\n","\n","Note that of course this method of tracking gradients is **much** more limited than what PyTorch offers. However it allows us to implement the backpropagation algorithm very simply and really see how it works."]},{"cell_type":"markdown","metadata":{"id":"CrrboKeZwMQm"},"source":["Let's set up some testing instrumentation:"]},{"cell_type":"code","metadata":{"id":"pNP2w3RgwMQn","executionInfo":{"status":"ok","timestamp":1639405751080,"user_tz":-120,"elapsed":1102,"user":{"displayName":"Shir Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxzgFCwcO2VgDEyrWnLAS5HsXbQDYe4JILsIFVgN4=s64","userId":"04622715026667402457"}}},"source":["from hw2.grad_compare import compare_layer_to_torch\n","\n","def test_block_grad(block: layers.Layer, x, y=None, delta=1e-3):\n","    diffs = compare_layer_to_torch(block, x, y)\n","    \n","    # Assert diff values\n","    for diff in diffs:\n","        test.assertLess(diff, delta)\n","\n","# Show the compare function\n","compare_layer_to_torch??"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fpdDuE8dwMQn"},"source":["Notes:\n","- After you complete your implementation, you should make sure to read and understand the `compare_layer_to_torch()` function. It will help you understand what PyTorch is doing.\n","- The value of `delta` above is should not be needed. A correct implementation will give you a `diff` of exactly zero."]},{"cell_type":"markdown","metadata":{"id":"LsqW5J-hwMQn"},"source":["## Layer Implementations\n","<a id=part1_2></a>\n","\n","We'll now implement some `Layer`s that will enable us to later build an MLP model of arbitrary depth, complete with automatic differentiation.\n","\n","For each block, you'll first implement the `forward()` function.\n","Then, you will calculate the derivative of the block by hand with respect to each of its\n","input tensors and each of its parameter tensors (if any).\n","Using your manually-calculated derivation, you can then implement the `backward()` function.\n","\n","Notice that we have intermediate Jacobians that are potentially high dimensional tensors.\n","For example in the expression\n","$\\pderiv{L}{\\vec{w}} = \\pderiv{L}{\\vec{z}}\\cdot \\pderiv{\\vec{z}}{\\vec{w}}$,\n","the term $\\pderiv{\\vec{z}}{\\vec{w}}$ is a 4D Jacobian if both $\\vec{z}$ and $\\vec{w}$\n","are 2D matrices.\n","\n","In order to implement the backpropagation algorithm efficiently,\n","we need to implement every backward function without explicitly constructing this \n","Jacobian. Instead, we're interested in directly calculating the vector-Jacobian product\n","(VJP) $\\pderiv{L}{\\vec{z}}\\cdot \\pderiv{\\vec{z}}{\\vec{w}}$. \n","In order to do this, you should try to figure out the gradient of the loss with respect to\n","one element, e.g. $\\pderiv{L}{\\vec{w}_{1,1}}$ and extrapolate from there how to\n","directly obtain the VJP."]},{"cell_type":"markdown","metadata":{"id":"1WLWl7CPwMQo"},"source":["### Activation functions"]},{"cell_type":"markdown","metadata":{"id":"XY1SUkPKwMQo"},"source":["#### (Leaky) ReLU\n","\n","ReLU, or rectified linear unit is a very common activation function in deep learning architectures.\n","In it's most standard form, as we'll implement here, it has no parameters.\n","\n","We'll first implement the \"leaky\" version, defined as\n","\n","$$\n","\\mathrm{relu}(\\vec{x}) = \\max(\\alpha\\vec{x},\\vec{x}), \\ 0\\leq\\alpha<1\n","$$\n","\n","This is similar to the ReLU activation we've seen in class, only that it has a small non-zero slope then it's input is negative.\n","Note that it's not strictly differentiable, however it has sub-gradients, defined separately any positive-valued input and for negative-valued input."]},{"cell_type":"markdown","metadata":{"id":"yaS4sGeewMQo"},"source":["**TODO**: Complete the implementation of the `LeakyReLU` class in the `hw2/layers.py` module."]},{"cell_type":"code","metadata":{"id":"3dcsK2VpwMQp","executionInfo":{"status":"ok","timestamp":1639405757936,"user_tz":-120,"elapsed":405,"user":{"displayName":"Shir Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxzgFCwcO2VgDEyrWnLAS5HsXbQDYe4JILsIFVgN4=s64","userId":"04622715026667402457"}}},"source":["N = 100\n","in_features = 200\n","num_classes = 10\n","eps = 1e-6"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pDQSloeEwMQp","executionInfo":{"status":"ok","timestamp":1639405758727,"user_tz":-120,"elapsed":347,"user":{"displayName":"Shir Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxzgFCwcO2VgDEyrWnLAS5HsXbQDYe4JILsIFVgN4=s64","userId":"04622715026667402457"}},"outputId":"cd9f3554-8a69-4126-f5df-252c910c351d"},"source":["# Test LeakyReLU\n","alpha = 0.1\n","lrelu = layers.LeakyReLU(alpha=alpha)\n","x_test = torch.randn(N, in_features)\n","\n","# Test forward pass\n","z = lrelu(x_test)\n","test.assertSequenceEqual(z.shape, x_test.shape)\n","test.assertTrue(torch.allclose(z, torch.nn.LeakyReLU(alpha)(x_test), atol=eps))\n","\n","# Test backward pass\n","test_block_grad(lrelu, x_test)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Comparing gradients... \n","input    diff=0.000\n"]}]},{"cell_type":"markdown","metadata":{"id":"WT4T4O-fwMQp"},"source":["Now using the LeakyReLU, we can trivially define a regular ReLU block as a special case.\n","\n","**TODO**: Complete the implementation of the `ReLU` class in the `hw2/layers.py` module."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mJ6t3LFdwMQq","executionInfo":{"status":"ok","timestamp":1639405761236,"user_tz":-120,"elapsed":3,"user":{"displayName":"Shir Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxzgFCwcO2VgDEyrWnLAS5HsXbQDYe4JILsIFVgN4=s64","userId":"04622715026667402457"}},"outputId":"c45ee9af-8655-45ea-d04c-ed17252fb32f"},"source":["# Test ReLU\n","relu = layers.ReLU()\n","x_test = torch.randn(N, in_features)\n","\n","# Test forward pass\n","z = relu(x_test)\n","test.assertSequenceEqual(z.shape, x_test.shape)\n","test.assertTrue(torch.allclose(z, torch.relu(x_test), atol=eps))\n","\n","# Test backward pass\n","test_block_grad(relu, x_test)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Comparing gradients... \n","input    diff=0.000\n"]}]},{"cell_type":"markdown","metadata":{"id":"vXdreT-vwMQq"},"source":["#### Sigmoid\n","\n","The sigmoid function $\\sigma(x)$ is also sometimes used as an activation function.\n","We have also seen it previously in the context of logistic regression.\n","\n","The sigmoid function is defined as\n","\n","$$\n","\\sigma(\\vec{x}) = \\frac{1}{1+\\exp(-\\vec{x})}.\n","$$"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eOCdu0JxsZqA","executionInfo":{"status":"ok","timestamp":1639405765072,"user_tz":-120,"elapsed":381,"user":{"displayName":"Shir Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxzgFCwcO2VgDEyrWnLAS5HsXbQDYe4JILsIFVgN4=s64","userId":"04622715026667402457"}},"outputId":"bcd9471c-f103-44cf-bbd1-9117a58cc564"},"source":["x_test = torch.randn(N, in_features)\n","1/x_test"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.5190,  0.6724,  1.1102,  ...,  2.7929,  2.0887,  0.7387],\n","        [ 1.9009,  0.4735, -1.9203,  ..., -3.0123, -1.2382,  1.2131],\n","        [40.4860, -0.9397, -1.3155,  ...,  3.1949,  1.2422, -0.8982],\n","        ...,\n","        [-2.6625,  0.7073,  4.3095,  ..., -1.2481,  0.7092, -1.1125],\n","        [-0.8991,  0.9244,  2.6822,  ..., -3.2524,  0.7326,  1.9921],\n","        [-1.2024,  1.5394, 59.9697,  ...,  1.5202, -1.4870, -0.7782]])"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NWmts2L6wMQq","executionInfo":{"status":"ok","timestamp":1639405773719,"user_tz":-120,"elapsed":693,"user":{"displayName":"Shir Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxzgFCwcO2VgDEyrWnLAS5HsXbQDYe4JILsIFVgN4=s64","userId":"04622715026667402457"}},"outputId":"cd805014-d0e5-4df8-b63d-6ff402ab8053"},"source":["# Test Sigmoid\n","sigmoid = layers.Sigmoid()\n","x_test = torch.randn(N, in_features, in_features) # 3D input should work\n","\n","# Test forward pass\n","z = sigmoid(x_test)\n","test.assertSequenceEqual(z.shape, x_test.shape)\n","test.assertTrue(torch.allclose(z, torch.sigmoid(x_test), atol=eps))\n","\n","# Test backward pass\n","test_block_grad(sigmoid, x_test)"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Comparing gradients... \n","input    diff=0.000\n"]}]},{"cell_type":"markdown","metadata":{"id":"q_UJDswDwMQq"},"source":["#### Hyperbolic Tangent\n","\n","The hyperbolic tangent function $\\tanh(x)$ is a common activation function used when the output should be in the range \\[-1, 1\\].\n","\n","The tanh function is defined as\n","\n","$$\n","\\tanh(\\vec{x}) = \\frac{\\exp(x)-\\exp(-x)}{\\exp(x)+\\exp(-\\vec{x})}.\n","$$"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Ac7xCNYwMQr","executionInfo":{"status":"ok","timestamp":1639405776982,"user_tz":-120,"elapsed":381,"user":{"displayName":"Shir Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxzgFCwcO2VgDEyrWnLAS5HsXbQDYe4JILsIFVgN4=s64","userId":"04622715026667402457"}},"outputId":"4f98677b-3da4-4878-b5ab-62661371770e"},"source":["# Test TanH\n","tanh = layers.TanH()\n","x_test = torch.randn(N, in_features, in_features) # 3D input should work\n","\n","# Test forward pass\n","z = tanh(x_test)\n","test.assertSequenceEqual(z.shape, x_test.shape)\n","test.assertTrue(torch.allclose(z, torch.tanh(x_test), atol=eps))\n","\n","# Test backward pass\n","test_block_grad(tanh, x_test)"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Comparing gradients... \n","input    diff=0.000\n"]}]},{"cell_type":"markdown","metadata":{"id":"bQ9qxcLZwMQr"},"source":["### Linear (fully connected) layer\n","\n","First, we'll implement an affine transform layer, also known as a fully connected layer.\n","\n","Given an input $\\mat{X}$ the layer computes,\n","\n","$$\n","\\mat{Z} = \\mat{X} \\mattr{W}  + \\vec{b} ,~\n","\\mat{X}\\in\\set{R}^{N\\times D_{\\mathrm{in}}},~\n","\\mat{W}\\in\\set{R}^{D_{\\mathrm{out}}\\times D_{\\mathrm{in}}},~ \\vec{b}\\in\\set{R}^{D_{\\mathrm{out}}}.\n","$$\n","\n","Notes:\n","- We write it this way to follow the implementation conventions.\n","- $N$ is the number of samples in the input (batch size). The input $\\mat{X}$ will always be a tensor containing a batch dimension first.\n","- Thanks to broadcasting, $\\vec{b}$ can remain a vector even though the input $\\mat{X}$ is a matrix.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WzO_AvZywMQr"},"source":["**TODO**: Complete the implementation of the `Linear` class in the `hw2/layers.py` module."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NjNT9m2CwMQr","executionInfo":{"status":"ok","timestamp":1639405780350,"user_tz":-120,"elapsed":336,"user":{"displayName":"Shir Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxzgFCwcO2VgDEyrWnLAS5HsXbQDYe4JILsIFVgN4=s64","userId":"04622715026667402457"}},"outputId":"9313ba7b-046e-4aac-f3f1-d317201006a8"},"source":["# Test Linear\n","out_features = 1000\n","fc = layers.Linear(in_features, out_features)\n","x_test = torch.randn(N, in_features)\n","\n","# Test forward pass\n","z = fc(x_test)\n","test.assertSequenceEqual(z.shape, [N, out_features])\n","torch_fc = torch.nn.Linear(in_features, out_features,bias=True)\n","torch_fc.weight = torch.nn.Parameter(fc.w)\n","torch_fc.bias = torch.nn.Parameter(fc.b)\n","test.assertTrue(torch.allclose(torch_fc(x_test), z, atol=eps))\n","\n","# Test backward pass\n","test_block_grad(fc, x_test)\n","\n","# Test second backward pass\n","x_test = torch.randn(N, in_features)\n","z = fc(x_test)\n","z = fc(x_test)\n","test_block_grad(fc, x_test)"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Comparing gradients... \n","input    diff=0.000\n","param#01 diff=0.000\n","param#02 diff=0.000\n","Comparing gradients... \n","input    diff=0.000\n","param#01 diff=0.000\n","param#02 diff=0.000\n"]}]},{"cell_type":"markdown","metadata":{"id":"xKtML4H8wMQs"},"source":["### Cross-Entropy Loss\n","\n","As you know by know, cross-entropy is a common loss function for classification tasks.\n","In class, we defined it as \n","\n","$$\\ell_{\\mathrm{CE}}(\\vec{y},\\hat{\\vec{y}}) = - {\\vectr{y}} \\log(\\hat{\\vec{y}})$$\n","\n","where $\\hat{\\vec{y}} = \\mathrm{softmax}(x)$ is a probability vector (the output of softmax on the class scores $\\vec{x}$) and the vector $\\vec{y}$ is a 1-hot encoded class label.\n","\n","However, it's tricky to compute the gradient of softmax, so instead we'll define a version of cross-entropy that produces the exact same output but works directly on the class scores $\\vec{x}$.\n","\n","We can write,\n","$$\\begin{align}\n","\\ell_{\\mathrm{CE}}(\\vec{y},\\hat{\\vec{y}}) &= - {\\vectr{y}} \\log(\\hat{\\vec{y}}) \n","= - {\\vectr{y}} \\log\\left(\\mathrm{softmax}(\\vec{x})\\right) \\\\\n","&= - {\\vectr{y}} \\log\\left(\\frac{e^{\\vec{x}}}{\\sum_k e^{x_k}}\\right) \\\\\n","&= - \\log\\left(\\frac{e^{x_y}}{\\sum_k e^{x_k}}\\right) \\\\\n","&= - \\left(\\log\\left(e^{x_y}\\right) - \\log\\left(\\sum_k e^{x_k}\\right)\\right)\\\\\n","&= - x_y + \\log\\left(\\sum_k e^{x_k}\\right)\n","\\end{align}$$\n","\n","Where the scalar $y$ is the correct class label, so $x_y$ is the correct class score.\n","\n","Note that this version of cross entropy is also what's [provided](https://pytorch.org/docs/stable/nn.html#crossentropyloss) by PyTorch's `nn` module."]},{"cell_type":"markdown","metadata":{"id":"OiwCoqruwMQs"},"source":["**TODO**: Complete the implementation of the `CrossEntropyLoss` class in the `hw2/layers.py` module."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"swZqvCtKwMQs","executionInfo":{"status":"ok","timestamp":1639405797958,"user_tz":-120,"elapsed":339,"user":{"displayName":"Shir Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxzgFCwcO2VgDEyrWnLAS5HsXbQDYe4JILsIFVgN4=s64","userId":"04622715026667402457"}},"outputId":"5cc21277-d54d-4d0d-8a7a-f22b5b0c9d32"},"source":["# Test CrossEntropy\n","cross_entropy = layers.CrossEntropyLoss()\n","scores = torch.randn(N, num_classes)\n","labels = torch.randint(low=0, high=num_classes, size=(N,), dtype=torch.long)\n","\n","# Test forward pass\n","loss = cross_entropy(scores, labels)\n","expected_loss = torch.nn.functional.cross_entropy(scores, labels)\n","test.assertLess(torch.abs(expected_loss-loss).item(), 1e-5)\n","print('loss=', loss.item())\n","\n","# Test backward pass\n","test_block_grad(cross_entropy, scores, y=labels)"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["loss= 2.7283618450164795\n","Comparing gradients... \n","input    diff=0.000\n"]}]},{"cell_type":"markdown","metadata":{"id":"Qr7ADuZ8wMQt"},"source":["## Building Models\n","<a id=part1_3></a>\n","\n","Now that we have some working `Layer`s, we can build an MLP model of arbitrary depth and compute end-to-end gradients.\n","\n","First, lets copy an idea from PyTorch and implement our own version of the `nn.Sequential` `Module`.\n","This is a `Layer` which contains other `Layer`s and calls them in sequence. We'll use this to build our MLP model."]},{"cell_type":"markdown","metadata":{"id":"1m_mBB7nwMQt"},"source":["**TODO**: Complete the implementation of the `Sequential` class in the `hw2/layers.py` module."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"87sTCLzGwMQt","executionInfo":{"status":"ok","timestamp":1639405802363,"user_tz":-120,"elapsed":364,"user":{"displayName":"Shir Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxzgFCwcO2VgDEyrWnLAS5HsXbQDYe4JILsIFVgN4=s64","userId":"04622715026667402457"}},"outputId":"73c3d282-9cd7-4061-e4d2-4d71c5647deb"},"source":["# Test Sequential\n","# Let's create a long sequence of layers and see\n","# whether we can compute end-to-end gradients of the whole thing.\n","\n","seq = layers.Sequential(\n","    layers.Linear(in_features, 100),\n","    layers.Linear(100, 200),\n","    layers.Linear(200, 100),\n","    layers.ReLU(),\n","    layers.Linear(100, 500),\n","    layers.LeakyReLU(alpha=0.01),\n","    layers.Linear(500, 200),\n","    layers.ReLU(),\n","    layers.Linear(200, 500),\n","    layers.LeakyReLU(alpha=0.1),\n","    layers.Linear(500, 1),\n","    layers.Sigmoid(),\n",")\n","x_test = torch.randn(N, in_features)\n","\n","# Test forward pass\n","z = seq(x_test)\n","test.assertSequenceEqual(z.shape, [N, 1])\n","\n","# Test backward pass\n","test_block_grad(seq, x_test)"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Comparing gradients... \n","input    diff=0.000\n","param#01 diff=0.000\n","param#02 diff=0.000\n","param#03 diff=0.000\n","param#04 diff=0.000\n","param#05 diff=0.000\n","param#06 diff=0.000\n","param#07 diff=0.000\n","param#08 diff=0.000\n","param#09 diff=0.000\n","param#10 diff=0.000\n","param#11 diff=0.000\n","param#12 diff=0.000\n","param#13 diff=0.000\n","param#14 diff=0.000\n"]}]},{"cell_type":"markdown","metadata":{"id":"mLE8XyzPwMQt"},"source":["Now, equipped with a `Sequential`, all we have to do is create an MLP architecture.\n","We'll define our MLP with the following hyperparameters:\n","- Number of input features, $D$.\n","- Number of output classes, $C$.\n","- Sizes of hidden layers, $h_1,\\dots,h_L$.\n","\n","So the architecture will be:\n","\n","FC($D$, $h_1$) $\\rightarrow$ ReLU $\\rightarrow$\n","FC($h_1$, $h_2$) $\\rightarrow$ ReLU $\\rightarrow$\n","$\\cdots$ $\\rightarrow$\n","FC($h_{L-1}$, $h_L$) $\\rightarrow$ ReLU $\\rightarrow$\n","FC($h_{L}$, $C$)\n","\n","We'll also create a sequence of the above MLP and a cross-entropy loss, since it's the gradient of the loss that we need in order to train a model."]},{"cell_type":"markdown","metadata":{"id":"01_cIrVrwMQu"},"source":["**TODO**: Complete the implementation of the `MLP` class in the `hw2/layers.py` module. Ignore the `dropout` parameter for now."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BTdxkFV-wMQu","executionInfo":{"status":"ok","timestamp":1639405807765,"user_tz":-120,"elapsed":356,"user":{"displayName":"Shir Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxzgFCwcO2VgDEyrWnLAS5HsXbQDYe4JILsIFVgN4=s64","userId":"04622715026667402457"}},"outputId":"566bacda-f62b-4af7-c6c2-0f9542a94d17"},"source":["# Create an MLP model\n","mlp = layers.MLP(in_features, num_classes, hidden_features=[100, 50, 100])\n","print(mlp)"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["MLP, Sequential\n","\t[0] Linear(200, 100)\n","\t[1] ReLU\n","\t[2] Linear(100, 50)\n","\t[3] ReLU\n","\t[4] Linear(50, 100)\n","\t[5] ReLU\n","\t[6] Linear(100, 10)\n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Mgd-xhiwMQu","executionInfo":{"status":"ok","timestamp":1639405814201,"user_tz":-120,"elapsed":351,"user":{"displayName":"Shir Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxzgFCwcO2VgDEyrWnLAS5HsXbQDYe4JILsIFVgN4=s64","userId":"04622715026667402457"}},"outputId":"ecbd177f-660b-49da-be10-1d3ee215979d"},"source":["# Test MLP architecture\n","N = 100\n","in_features = 10\n","num_classes = 10\n","for activation in ('relu', 'sigmoid'):\n","    mlp = layers.MLP(in_features, num_classes, hidden_features=[100, 50, 100], activation=activation)\n","    test.assertEqual(len(mlp.sequence), 7)\n","    \n","    num_linear = 0\n","    for b1, b2 in zip(mlp.sequence, mlp.sequence[1:]):\n","        if (str(b2).lower() == activation):\n","            test.assertTrue(str(b1).startswith('Linear'))\n","            num_linear += 1\n","            \n","    test.assertTrue(str(mlp.sequence[-1]).startswith('Linear'))\n","    test.assertEqual(num_linear, 3)\n","\n","    # Test MLP gradients\n","    # Test forward pass\n","    x_test = torch.randn(N, in_features)\n","    labels = torch.randint(low=0, high=num_classes, size=(N,), dtype=torch.long)\n","    z = mlp(x_test)\n","    test.assertSequenceEqual(z.shape, [N, num_classes])\n","\n","    # Create a sequence of MLPs and CE loss\n","    seq_mlp = layers.Sequential(mlp, layers.CrossEntropyLoss())\n","    loss = seq_mlp(x_test, y=labels)\n","    test.assertEqual(loss.dim(), 0)\n","    print(f'MLP loss={loss}, activation={activation}')\n","\n","    # Test backward pass\n","    test_block_grad(seq_mlp, x_test, y=labels)"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["MLP loss=2.300506353378296, activation=relu\n","Comparing gradients... \n","input    diff=0.000\n","param#01 diff=0.000\n","param#02 diff=0.000\n","param#03 diff=0.000\n","param#04 diff=0.000\n","param#05 diff=0.000\n","param#06 diff=0.000\n","param#07 diff=0.000\n","param#08 diff=0.000\n","MLP loss=2.300757646560669, activation=sigmoid\n","Comparing gradients... \n","input    diff=0.000\n","param#01 diff=0.000\n","param#02 diff=0.000\n","param#03 diff=0.000\n","param#04 diff=0.000\n","param#05 diff=0.000\n","param#06 diff=0.000\n","param#07 diff=0.000\n","param#08 diff=0.000\n"]}]},{"cell_type":"markdown","metadata":{"id":"nSA1jfxXwMQu"},"source":["If the above tests passed then congratulations - you've now implemented an arbitrarily deep model and loss function with end-to-end automatic differentiation!"]},{"cell_type":"markdown","metadata":{"id":"zVscr41MwMQu"},"source":["## Questions\n","<a id=part2_7></a>"]},{"cell_type":"markdown","metadata":{"id":"40wTUbDCwMQv"},"source":["**TODO** Answer the following questions. Write your answers in the appropriate variables in the module `hw2/answers.py`."]},{"cell_type":"code","metadata":{"id":"51jyTyWtwMQv","executionInfo":{"status":"ok","timestamp":1639405820200,"user_tz":-120,"elapsed":2146,"user":{"displayName":"Shir Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxzgFCwcO2VgDEyrWnLAS5HsXbQDYe4JILsIFVgN4=s64","userId":"04622715026667402457"}}},"source":["from cs3600.answers import display_answer\n","import hw2.answers"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7NmgnsB0wMQv"},"source":["### Question 1 \n","\n","Suppose we have a linear (i.e. fully-connected) layer, defined with `in_features=1024` and `out_features=2048`. We apply this layer to an input tensor $\\mat{X}$ containing a batch of `N=128` samples.\n","\n","1. What would then be the shape of the Jacobian tensor of the output of the layer w.r.t. the input $\\mat{X}$?\n","\n","2. Assuming we're using single-precision floating point (32 bits) to represent our tensors, How many gigabytes of RAM or GPU memory will be required to store the above Jacobian?"]},{"cell_type":"code","metadata":{"id":"cbrFJquRwMQv","colab":{"base_uri":"https://localhost:8080/","height":161},"executionInfo":{"status":"ok","timestamp":1639405821818,"user_tz":-120,"elapsed":8,"user":{"displayName":"Shir Cohen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxzgFCwcO2VgDEyrWnLAS5HsXbQDYe4JILsIFVgN4=s64","userId":"04622715026667402457"}},"outputId":"21227840-4e6a-48f0-9461-5ccf619620cd"},"source":["display_answer(hw2.answers.part1_q1)"],"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/markdown":"\n1. For a single sample the Jacobian matrix is of size (2048, 1024) - (output dim, input dim). When X is a tensor with batch size 128 \nthis adds 2 dimension to the Jacobian which would bring the size to (128, 2048, 128, 1024).\n<br><br>\n2. The number of parameters in the matrix is $ 128 * 2048 * 128 * 1024 = 68719476736\n$. <br> Each parameter takes up 32 bits\nso in total we would be using $ 32 * 68719476736 $ bits. <br>\nIn gigabytes that is: $ \\frac{(32 * 68719476736)}{1024^3} = 2048 $ gigabytes\n\n\n","text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{}}]}]}